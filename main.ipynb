{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "# https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri/data\n",
    "# !kaggle datasets download sartajbhuvaji/brain-tumor-classification-mri\n",
    "# !unzip \"./brain-tumor-classification-mri.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, precision_score\n",
    "import numpy as np\n",
    "import torchvision.transforms as v2 \n",
    "import albumentations as A\n",
    "import mlflow\n",
    "from aux import ImagesMRIDataset, split_for_cross_validation, get_training_testing_data, split_traing_data \n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, path : str, patience=5, threshold=1e-4):\n",
    "        self.patience = patience\n",
    "        self.threshold = threshold\n",
    "        self.min_loss = 10000\n",
    "        self.steps_till_stop = 0\n",
    "        self.path = path\n",
    "\n",
    "    def continue_training(self, model, loss):\n",
    "        if(loss < self.min_loss - self.threshold):\n",
    "            self.min_loss = loss\n",
    "            self.steps_till_stop = 0\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            return True\n",
    "        if (loss >= self.min_loss - self.threshold):\n",
    "            self.steps_till_stop += 1\n",
    "            if (self.steps_till_stop == self.patience): return False\n",
    "        return True\n",
    "    \n",
    "    def load_model(self, model):\n",
    "        model.load_state_dict(torch.load(self.path, weights_only=True))\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, criterion, optimizer, dataloader : DataLoader):\n",
    "   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   current_training_loss = 0\n",
    "   all_train_labels, all_train_preds = [], []\n",
    "   model.train()\n",
    "   for idx, (images, labels) in enumerate(dataloader):\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      output = model(images)\n",
    "      output = output.to(device)\n",
    "      loss = criterion(output, labels)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      current_training_loss += loss.item()\n",
    "      all_train_preds.extend(output.argmax(dim=1).cpu().numpy())\n",
    "      all_train_labels.extend(labels.cpu().numpy())\n",
    "   loss = current_training_loss / len(dataloader)\n",
    "   acc = round(accuracy_score(all_train_preds, all_train_labels), 3)\n",
    "   return loss, acc\n",
    "\n",
    "def validation_loop(model, criterion, dataloader : DataLoader):\n",
    "   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   all_val_labels, all_val_preds = [], [],\n",
    "   current_validation_loss = 0\n",
    "   model.eval()\n",
    "   with torch.no_grad():\n",
    "      for idx, (images, labels) in enumerate(dataloader):\n",
    "         images, labels = images.to(device), labels.to(device)\n",
    "         output = model(images)\n",
    "         output = output.to(device)\n",
    "         loss = criterion(output, labels)\n",
    "         current_validation_loss += loss.item()\n",
    "         all_val_labels.extend(labels.cpu().numpy())\n",
    "         all_val_preds.extend(output.argmax(dim=1).cpu().numpy())\n",
    "   \n",
    "   loss = current_validation_loss / len(dataloader)\n",
    "   acc =  round(accuracy_score(all_val_labels, all_val_preds), 3)\n",
    "   return loss, acc,   \n",
    "\n",
    "def train_model(parameters, model, data):\n",
    "  earlyStopping : EarlyStopping = parameters[\"early_stopping\"]\n",
    "  current_acc = 0.0\n",
    "  for i in range(parameters[\"epochs\"]):\n",
    "      tloss, tacc = training_loop(model, parameters[\"criterion\"], parameters[\"optimizer\"], data[\"train\"])\n",
    "      mlflow.log_metrics(\n",
    "      metrics={\n",
    "            \"train_loss\": tloss,\n",
    "            \"train_accuracy\": tacc\n",
    "         },\n",
    "         step=i\n",
    "      )\n",
    "      vloss, vacc = validation_loop(model, parameters[\"criterion\"], data[\"validation\"])\n",
    "      mlflow.log_metrics(\n",
    "      metrics={\n",
    "          \"validation_loss\": vloss,\n",
    "          \"validation_accuracy\": vacc,\n",
    "          },\n",
    "          step=i\n",
    "      )\n",
    "      if (earlyStopping != None and not earlyStopping.continue_training(model, vloss)):\n",
    "         print(\"Acuratetea nu a crescut de ceva vreme, a intervenit early stopping\")\n",
    "         break\n",
    "      if (vacc > current_acc): \n",
    "         current_acc = vacc\n",
    "         print(f\"Acuratetea maxima {vacc}, la epoca {i}, loss de {vloss}\")\n",
    "      else:\n",
    "         print(f\"acuratete {vacc}, epoca {i}, loss {vloss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,  width : int, expansion : int):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.width1 = width\n",
    "        self.conv1 = nn.Conv2d(3, self.width1, kernel_size=(3, 3))\n",
    "        self.bn1 = nn.BatchNorm2d(self.width1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "        self.width2 = self.width1 * expansion\n",
    "        self.conv2 = nn.Conv2d(self.width1, self.width2, kernel_size=(3, 3))\n",
    "        self.bn2 = nn.BatchNorm2d(self.width2)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "        self.width3 = self.width2 * expansion\n",
    "        self.conv3 = nn.Conv2d(self.width2, self.width3, kernel_size=(5, 5))\n",
    "        self.bn3 = nn.BatchNorm2d(self.width3)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "        self.width4 = self.width3 * expansion\n",
    "        self.conv4 = nn.Conv2d(self.width3, self.width4, kernel_size=(3, 3))\n",
    "        self.bn4 = nn.BatchNorm2d(self.width4)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "        self.width5 = self.width4 * expansion\n",
    "        self.conv5 = nn.Conv2d(self.width4, self.width5, kernel_size=(3, 3))\n",
    "        self.bn5 = nn.BatchNorm2d(self.width5)\n",
    "\n",
    "        # (1024, 12) -> (1024, 1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classification_layer = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(self.width5, self.width4),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.width4, self.width3),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.width3, 4),\n",
    "            # nn.Softmax(dim=1)\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.maxpool1(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.maxpool2(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.maxpool3(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.maxpool4(self.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.avgpool(self.relu(self.bn5(self.conv5(x))))\n",
    "        return self.classification_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_validation_data(parameters, data_chunks, test_data, current : int):\n",
    "    training_data = []\n",
    "    for idx, chunk in enumerate(data_chunks):\n",
    "        if (idx == current):\n",
    "            continue\n",
    "        training_data += chunk\n",
    "    return {\n",
    "        \"train\": DataLoader(ImagesMRIDataset(training_data, transformations=parameters[\"train_transform\"]), batch_size=parameters[\"batch_size\"], shuffle=True, drop_last=True),\n",
    "        \"validation\": DataLoader(ImagesMRIDataset(data_chunks[current], transformations=parameters[\"test_transform\"]), batch_size=parameters[\"batch_size\"], shuffle=True),\n",
    "        \"test\": DataLoader(ImagesMRIDataset(test_data, transformations=parameters[\"test_transform\"]), batch_size=parameters[\"batch_size\"], shuffle=True)\n",
    "    }\n",
    "\n",
    "def compute_cross_validation(parameters, K, data_chunks, test_info):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for idx in range(0, K):\n",
    "        if (idx > 0): return\n",
    "        print(f\"Antrenare fold {idx}:\")\n",
    "        with mlflow.start_run(run_name=f\"fold_{idx}\") as run:\n",
    "            model = Net(width=parameters[\"width\"], expansion=parameters[\"expansion\"])\n",
    "            parameters[\"optimizer\"] = torch.optim.Adam(model.parameters(), lr=parameters[\"lr\"], weight_decay=parameters[\"weight_decay\"])\n",
    "            model = model.to(device)\n",
    "            mlflow.log_params(parameters)\n",
    "            data = create_cross_validation_data(parameters, data_chunks, test_info, idx)\n",
    "            train_model(parameters, model, data)\n",
    "            if (parameters[\"early_stopping\"]  != None):\n",
    "                best_model = Net(width=parameters[\"width\"], expansion=parameters[\"expansion\"])\n",
    "                parameters[\"early_stopping\"].load_model(best_model)\n",
    "            else:\n",
    "                best_model = model \n",
    "            loss, acc = validation_loop(best_model, parameters[\"criterion\"], data[\"test\"])\n",
    "            mlflow.log_metrics(\n",
    "                metrics={\n",
    "              \"test_loss\": loss,\n",
    "              \"test_accuracy\": acc,\n",
    "              }\n",
    "            )\n",
    "            print(f\"pentru test: acuratete {acc}, pierderea este {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antrenare fold 0:\n",
      "Acuratetea maxima 0.517, la epoca 0, loss de 1.1574879126115278\n",
      "Acuratetea maxima 0.593, la epoca 1, loss de 1.0238515680486506\n",
      "Acuratetea maxima 0.605, la epoca 2, loss de 0.9415839368646796\n",
      "True\n",
      "pentru test: acuratete 0.307, pierderea este 1.5513173852648054\n",
      "🏃 View run fold_0 at: http://localhost:5000/#/experiments/697423403428355787/runs/15546cf691864f7caec96d1e26f0e7d7\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/697423403428355787\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "K = 5\n",
    "BALANCED = True\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "# training_info, validation_info = split_traing_data(training_info, 0.2)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "\n",
    "training_no_transforms = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize((100, 100))\n",
    "])\n",
    "\n",
    "# EarlyStopping(path=\"./aici.pth\", patience=5)\n",
    "parameters = {\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\" : 3,\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"width\": 4,\n",
    "        \"expansion\" : 2,\n",
    "        \"train_transform\": training_no_transforms,\n",
    "        \"test_transform\": training_no_transforms,\n",
    "        \"criterion\": torch.nn.CrossEntropyLoss(),\n",
    "        \"optimizer\": None,\n",
    "        \"scheduler\": None,\n",
    "        \"early_stopping\": None,\n",
    "    }\n",
    "\n",
    "# task 1 - fara augmentari, duplicarea exemplelor din clasele suplimentare\n",
    "mlflow.set_experiment(experiment_name=\"Task1\")\n",
    "compute_cross_validation(parameters, K, data_chunks, test_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = (100, 100)\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=SIZE[0], width=SIZE[1]),   \n",
    "    A.HorizontalFlip(p = 0.5),\n",
    "    # A.CLAHE(clip_limit=5.0, tile_grid_size=(8, 8), p=1.0), \n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8), \n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "test_transformations = A.Compose([\n",
    "    A.Resize(height=SIZE[0], width=SIZE[1]),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "def compute_train_transformations(image):\n",
    "    image = train_transform(image=image)[\"image\"]\n",
    "    return np.array(image.transpose((2, 0, 1)), dtype=np.float32)\n",
    "    \n",
    "def compute_test_transformations(image):\n",
    "    image = test_transformations(image=image)[\"image\"]\n",
    "    return np.array(image.transpose((2, 0, 1)), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antrenare fold 0:\n",
      "Acuratetea maxima 0.25, la epoca 0, loss de 1.395501732826233\n",
      "Acuratetea maxima 0.504, la epoca 1, loss de 1.168650507926941\n",
      "Acuratetea maxima 0.618, la epoca 2, loss de 0.9428816636403402\n",
      "Acuratetea maxima 0.694, la epoca 3, loss de 0.7541070481141409\n",
      "Acuratetea maxima 0.725, la epoca 4, loss de 0.71403240164121\n",
      "Acuratetea maxima 0.75, la epoca 5, loss de 0.6779457628726959\n",
      "acuratete 0.735, epoca 6, loss 0.6409063637256622\n",
      "Acuratetea maxima 0.79, la epoca 7, loss de 0.567603588104248\n",
      "acuratete 0.774, epoca 8, loss 0.6490016480286916\n",
      "acuratete 0.748, epoca 9, loss 0.6551865041255951\n",
      "Acuratetea maxima 0.808, la epoca 10, loss de 0.47614013652006787\n",
      "Acuratetea maxima 0.816, la epoca 11, loss de 0.44285225371519726\n",
      "acuratete 0.742, epoca 12, loss 0.656170129776001\n",
      "Acuratetea maxima 0.827, la epoca 13, loss de 0.46151144802570343\n",
      "acuratete 0.805, epoca 14, loss 0.4709816525379817\n",
      "Acuratetea maxima 0.837, la epoca 15, loss de 0.4279339810212453\n",
      "Acuratetea maxima 0.86, la epoca 16, loss de 0.44110147655010223\n",
      "acuratete 0.849, epoca 17, loss 0.45902185638745624\n",
      "acuratete 0.846, epoca 18, loss 0.3958806296189626\n",
      "Acuratetea maxima 0.873, la epoca 19, loss de 0.3761360968152682\n",
      "True\n",
      "pentru test: acuratete 0.665, pierderea este 1.1023084437474608\n",
      "🏃 View run fold_0 at: http://localhost:5000/#/experiments/690686248894602373/runs/8dbb6c2b7289418f967684c7b4b9f74d\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/690686248894602373\n"
     ]
    }
   ],
   "source": [
    "K = 5\n",
    "BALANCED = True\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "\n",
    "# task 2 - cu augmentari, cu duplicarea exemplelor din clasele suplimentare\n",
    "parameters = {\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\" : 20,\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"width\": 4,\n",
    "        \"expansion\" : 2,\n",
    "        \"train_transform\": compute_train_transformations,\n",
    "        \"test_transform\": compute_test_transformations,\n",
    "        \"criterion\": torch.nn.CrossEntropyLoss(),\n",
    "        \"optimizer\": None,\n",
    "        \"scheduler\": None,\n",
    "        \"early_stopping\": None,\n",
    "    }\n",
    "\n",
    "mlflow.set_experiment(experiment_name=\"Task2\")\n",
    "compute_cross_validation(parameters, K, data_chunks, test_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.4758, 3.4916, 7.2595, 3.4705])\n",
      "Antrenare fold 0:\n",
      "Acuratetea maxima 0.165, la epoca 0, loss de 1.386326789855957\n",
      "Acuratetea maxima 0.286, la epoca 1, loss de 1.4179309010505676\n",
      "acuratete 0.286, epoca 2, loss 1.4051107466220856\n",
      "acuratete 0.286, epoca 3, loss 1.5790435075759888\n",
      "Acuratetea maxima 0.332, la epoca 4, loss de 1.3863252103328705\n",
      "Acuratetea maxima 0.477, la epoca 5, loss de 1.1966154277324677\n",
      "acuratete 0.306, epoca 6, loss 1.6896510124206543\n",
      "acuratete 0.364, epoca 7, loss 1.4237808883190155\n",
      "Acuratetea maxima 0.547, la epoca 8, loss de 1.1571098864078522\n",
      "acuratete 0.542, epoca 9, loss 1.1488657295703888\n",
      "acuratete 0.531, epoca 10, loss 1.2210025787353516\n",
      "Acuratetea maxima 0.67, la epoca 11, loss de 0.913056418299675\n",
      "acuratete 0.67, epoca 12, loss 0.9308577924966812\n",
      "Acuratetea maxima 0.688, la epoca 13, loss de 0.8520222008228302\n",
      "acuratete 0.547, epoca 14, loss 1.1130298674106598\n",
      "acuratete 0.675, epoca 15, loss 0.8412214368581772\n",
      "acuratete 0.61, epoca 16, loss 0.9440016746520996\n",
      "acuratete 0.551, epoca 17, loss 1.115492582321167\n",
      "acuratete 0.538, epoca 18, loss 1.1566521525382996\n",
      "acuratete 0.57, epoca 19, loss 0.9817870408296585\n",
      "pentru test: acuratete 0.404, pierderea este 1.4859105348587036\n",
      "🏃 View run fold_0 at: http://localhost:5000/#/experiments/386977441904780536/runs/b9d136e0be3749f4ab534e7bfc17e2aa\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/386977441904780536\n"
     ]
    }
   ],
   "source": [
    "K = 5\n",
    "BALANCED = False\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "training_info, validation_info = split_traing_data(training_info, 0.2)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "\n",
    "def compute_class_weights(data_stream):\n",
    "    labels = [label for _, label in data_stream]\n",
    "    label_counts = Counter(labels)\n",
    "    total_samples = sum(label_counts.values())\n",
    "    class_weights = {label: total_samples / count for label, count in label_counts.items()}\n",
    "    max_label = max(label_counts.keys())\n",
    "    weights_list = [class_weights.get(i, 0.0) for i in range(max_label + 1)]\n",
    "    weights = torch.tensor(weights_list, dtype=torch.float)\n",
    "    print(weights)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    weights = weights.to(device)\n",
    "    return weights\n",
    "\n",
    "parameters = {\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\" : 20,\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"width\": 4,\n",
    "        \"expansion\" : 2,\n",
    "        \"train_transform\": compute_train_transformations,\n",
    "        \"test_transform\": compute_test_transformations,\n",
    "        \"criterion\": torch.nn.CrossEntropyLoss(weight=compute_class_weights(training_info)),\n",
    "        \"optimizer\": None,\n",
    "        \"scheduler\": None,\n",
    "        \"early_stopping\": None,\n",
    "    } \n",
    "\n",
    "# Task 2 cu augmentari - cu functia de loss avand weights\n",
    "mlflow.set_experiment(experiment_name=\"Task2 augmentari\")\n",
    "compute_cross_validation(parameters, K, data_chunks, test_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import Resize, Rand2DElastic, RandAdjustContrast, RandRotate, ScaleIntensity, Compose\n",
    "from monai.transforms import RandGaussianNoise, RandAffine, RandStdShiftIntensity\n",
    "from monai.transforms import RandGaussianSmooth, ThresholdIntensity, RandAxisFlip\n",
    "from monai.transforms import RandSpatialCrop, RandGaussianSharpen, RandHistogramShift, RandShiftIntensity\n",
    "import cv2\n",
    "\n",
    "first_augmentation_pipeline = Compose([\n",
    "    Resize(spatial_size= SIZE),\n",
    "    RandGaussianSharpen(),\n",
    "    RandAxisFlip(prob=0.3),\n",
    "    RandSpatialCrop(roi_size=SIZE, random_center=True, random_size=False),\n",
    "    RandRotate(range_x=(-50, 50),  prob=0.5, keep_size=True),\n",
    "    RandAdjustContrast(prob=0.8),\n",
    "    ScaleIntensity(minv=0.0, maxv=1.0)\n",
    "])\n",
    "\n",
    "second_augmentation_pipeline = Compose([\n",
    "    Resize(spatial_size= SIZE),\n",
    "    Rand2DElastic(spacing=(5, 10), magnitude_range=(1, 2), prob=0.5, rotate_range=(0, 0), shear_range=(0.02, 0.02),\n",
    "                    translate_range=(10, 10), scale_range=(0.1, 0.1), spatial_size=SIZE, mode=\"bilinear\", padding_mode=\"reflection\"),\n",
    "    RandHistogramShift(prob=0.8),\n",
    "    ThresholdIntensity(threshold=20),\n",
    "    RandGaussianSmooth(sigma_x=(0.5, 1.5), sigma_y=(0.5, 1.5), prob=0.8),\n",
    "    RandRotate(range_x=(-50, 50),  prob=0.5, keep_size=True),\n",
    "    ScaleIntensity(minv=0.0, maxv=1.0)\n",
    "])\n",
    "\n",
    "\n",
    "third_augmentation_pipeline = Compose([\n",
    "    Resize(spatial_size= SIZE),\n",
    "    RandGaussianNoise(mean=0.0, std=0.1, prob=0.8),\n",
    "    RandShiftIntensity(offsets=(-20, 20), prob=0.3, safe=True),\n",
    "    RandAxisFlip(prob=0.3),\n",
    "    RandStdShiftIntensity(factors=0.8),\n",
    "    RandAffine(prob=0.2),\n",
    "    ScaleIntensity(minv=0.0, maxv=1.0)\n",
    "])\n",
    "\n",
    "test_augmentation_pipeline = Compose([\n",
    "    Resize(spatial_size= SIZE),\n",
    "    ScaleIntensity(minv=0.0, maxv=1.0)\n",
    "])\n",
    "\n",
    "def create_test_monai(img):\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return test_augmentation_pipeline(img)\n",
    "\n",
    "def create_first_monai_transform(img):\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return first_augmentation_pipeline(img)\n",
    "\n",
    "\n",
    "def create_second_monai_transform(img):\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return second_augmentation_pipeline(img)\n",
    "\n",
    "\n",
    "def create_third_monai_transform(img):\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return third_augmentation_pipeline(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "BALANCED = True\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "training_info, validation_info = split_traing_data(training_info, 0.2)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "\n",
    "parameters = {\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\" : 20,\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"width\": 4,\n",
    "        \"expansion\" : 2,\n",
    "        \"train_transform\": create_first_monai_transform,\n",
    "        \"test_transform\": create_test_monai,\n",
    "        \"criterion\": torch.nn.CrossEntropyLoss(),\n",
    "        \"optimizer\": None,\n",
    "        \"scheduler\": None,\n",
    "        \"early_stopping\": None,\n",
    "    } \n",
    "\n",
    "mlflow.set_experiment(experiment_name=\"Task3 set1 transformari\")\n",
    "compute_cross_validation(parameters, K, data_chunks, test_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\" : 20,\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"width\": 4,\n",
    "        \"expansion\" : 2,\n",
    "        \"train_transform\": create_second_monai_transform,\n",
    "        \"test_transform\": create_test_monai,\n",
    "        \"criterion\": torch.nn.CrossEntropyLoss(),\n",
    "        \"optimizer\": None,\n",
    "        \"scheduler\": None,\n",
    "        \"early_stopping\": None,\n",
    "    } \n",
    "mlflow.set_experiment(experiment_name=\"Task3 set2 transformari\")\n",
    "compute_cross_validation(parameters, K, data_chunks, test_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\" : 20,\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"width\": 4,\n",
    "        \"expansion\" : 2,\n",
    "        \"train_transform\": create_third_monai_transform,\n",
    "        \"test_transform\": create_test_monai,\n",
    "        \"criterion\": torch.nn.CrossEntropyLoss(),\n",
    "        \"optimizer\": None,\n",
    "        \"scheduler\": None,\n",
    "        \"early_stopping\": None,\n",
    "    } \n",
    "mlflow.set_experiment(experiment_name=\"Task3 set3 transformari\")\n",
    "compute_cross_validation(parameters, K, data_chunks, test_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_third_monai_transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      4\u001b[39m training_info, validation_info = split_traing_data(training_info, \u001b[32m0.2\u001b[39m)\n\u001b[32m      5\u001b[39m data_chunks = split_for_cross_validation(training_info, K)\n\u001b[32m      6\u001b[39m parameters = {\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m128\u001b[39m,\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m : \u001b[32m20\u001b[39m,\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1e-3\u001b[39m,\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1e-4\u001b[39m,\n\u001b[32m     11\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwidth\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m4\u001b[39m,\n\u001b[32m     12\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mexpansion\u001b[39m\u001b[33m\"\u001b[39m : \u001b[32m2\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtrain_transform\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mcreate_third_monai_transform\u001b[49m,\n\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtest_transform\u001b[39m\u001b[33m\"\u001b[39m: create_test_monai,\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcriterion\u001b[39m\u001b[33m\"\u001b[39m: torch.nn.CrossEntropyLoss(),\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     17\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mscheduler\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     18\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mearly_stopping\u001b[39m\u001b[33m\"\u001b[39m: EarlyStopping(\u001b[33m\"\u001b[39m\u001b[33m./aici.pth\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m5\u001b[39m)\n\u001b[32m     19\u001b[39m     } \n\u001b[32m     20\u001b[39m mlflow.set_experiment(experiment_name=\u001b[33m\"\u001b[39m\u001b[33mTask4 early stopper strategy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m compute_cross_validation(parameters, K, data_chunks, test_info)\n",
      "\u001b[31mNameError\u001b[39m: name 'create_third_monai_transform' is not defined"
     ]
    }
   ],
   "source": [
    "K = 5\n",
    "BALANCED = True\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "training_info, validation_info = split_traing_data(training_info, 0.2)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "parameters = {\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\" : 20,\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"width\": 4,\n",
    "        \"expansion\" : 2,\n",
    "        \"train_transform\": compute_train_transformations,\n",
    "        \"test_transform\": compute_test_transformations,\n",
    "        \"criterion\": torch.nn.CrossEntropyLoss(),\n",
    "        \"optimizer\": None,\n",
    "        \"scheduler\": None,\n",
    "        \"early_stopping\": EarlyStopping(\"./aici.pth\", 5)\n",
    "    } \n",
    "mlflow.set_experiment(experiment_name=\"Task4 early stopper strategy\")\n",
    "compute_cross_validation(parameters, K, data_chunks, test_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "BALANCED = True\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "training_info, validation_info = split_traing_data(training_info, 0.2)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "current_parameters = {\n",
    "    \"batch_size\": 50,\n",
    "    \"epochs\" : 25,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"optimizer_patience\" : 3,\n",
    "    \"lr_factor\": 0.6,\n",
    "    \"width\": 16,\n",
    "    \"expansion\" : 4,\n",
    "    \"early_stopping_patience\" : 7,\n",
    "}\n",
    "\n",
    "def compute_functions_with_lr_scheduler(model : Net, parameters):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=parameters[\"lr\"], weight_decay=parameters[\"weight_decay\"])\n",
    "    return torch.nn.CrossEntropyLoss(), optimizer, torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', \n",
    "                patience=parameters[\"optimizer_patience\"], factor=parameters[\"lr_factor\"]), None\n",
    "\n",
    "\n",
    "task4_lr_scheduler_test, task4_lr_scheduler_validation = compute_cross_validation(current_parameters, K, data_chunks, test_info, create_first_monai_transform, create_test_monai, compute_functions_with_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "BALANCED = True\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "training_info, validation_info = split_traing_data(training_info, 0.2)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "parameters = {\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\" : 50,\n",
    "    \"lr\": 1e-2,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"optimizer_patience\" : 3,\n",
    "    \"lr_factor\": 0.6,\n",
    "    \"width\": 16,\n",
    "    \"expansion\" : 4,\n",
    "    \"early_stopping_patience\" : 7,\n",
    "}\n",
    "\n",
    "# 72 maxim\n",
    "def compute_ablatiations(model : Net, parameters):\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=parameters[\"lr\"], weight_decay=parameters[\"weight_decay\"])\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=parameters[\"lr\"], weight_decay=parameters[\"weight_decay\"], momentum=0.7)\n",
    "    return torch.nn.BCEWithLogitsLoss(), optimizer, torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', \n",
    "                patience=parameters[\"optimizer_patience\"], factor=parameters[\"lr_factor\"]), EarlyStopping(\"./aici.pth\", patience=parameters[\"early_stopping_patience\"])\n",
    "\n",
    "SIZE=(100, 100)\n",
    "ablatiation_augmentation = Compose([\n",
    "    Resize(spatial_size= SIZE),\n",
    "    RandGaussianSharpen(),\n",
    "    RandAxisFlip(prob=0.3),\n",
    "    RandSpatialCrop(roi_size=SIZE, random_center=True, random_size=False),\n",
    "    RandRotate(range_x=(-50, 50),  prob=0.5, keep_size=True),\n",
    "    RandAdjustContrast(prob=0.8),\n",
    "    ScaleIntensity(minv=0.0, maxv=1.0)\n",
    "])\n",
    "\n",
    "\n",
    "def create_ablatiation_transform(img):\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return ablatiation_augmentation(img)\n",
    "\n",
    "\n",
    "test_augmentation_ablatiation = Compose([\n",
    "    Resize(spatial_size= SIZE),\n",
    "    ScaleIntensity(minv=0.0, maxv=1.0)\n",
    "])\n",
    "\n",
    "def create_ablatiation_test_monai(img):\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return test_augmentation_ablatiation(img)\n",
    "\n",
    "info_test = {\"precision\": [], \"recall\": [], \"f1Score\" : [], \"accuracy\" : []}\n",
    "for i in range(K):\n",
    "    if (i != 1):\n",
    "        continue\n",
    "    model = Net(width=parameters[\"width\"], expansion=parameters[\"expansion\"])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion, optimizer, lr_scheduler, early_stopping = compute_ablatiations(model, parameters)\n",
    "    data = create_cross_validation_data(data_chunks, test_info, i, create_ablatiation_transform, create_ablatiation_test_monai, parameters[\"epochs\"])\n",
    "    training_loss, validation_loss, training_accuracy, validation_accuracy = train_model(model, parameters[\"epochs\"],\n",
    "            data, criterion, optimizer, lr_scheduler, early_stopping)\n",
    "    if (early_stopping != None):\n",
    "        best_model = Net(width=parameters[\"width\"], expansion=parameters[\"expansion\"])\n",
    "        early_stopping.load_model(best_model)\n",
    "    else:\n",
    "        best_model = model\n",
    "    precision, recall, f1, acc, mat = test_model(best_model, data[\"validation\"], criterion)\n",
    "    precision, recall, f1, acc, mat = test_model(best_model, data[\"test\"], criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
