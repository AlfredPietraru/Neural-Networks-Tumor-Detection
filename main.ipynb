{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "# https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri/data\n",
    "# !kaggle datasets download sartajbhuvaji/brain-tumor-classification-mri\n",
    "# !unzip \"./brain-tumor-classification-mri.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, precision_score\n",
    "import numpy as np\n",
    "import torchvision.transforms as v2 \n",
    "import albumentations as A\n",
    "import mlflow\n",
    "from aux import ImagesMRIDataset, split_for_cross_validation, get_training_testing_data, split_traing_data \n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, path : str, patience=5, threshold=1e-4):\n",
    "        self.patience = patience\n",
    "        self.threshold = threshold\n",
    "        self.min_loss = 10000\n",
    "        self.steps_till_stop = 0\n",
    "        self.path = path\n",
    "\n",
    "    def continue_training(self, model, loss):\n",
    "        if(loss < self.min_loss - self.threshold):\n",
    "            self.min_loss = loss\n",
    "            self.steps_till_stop = 0\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            return True\n",
    "        if (loss >= self.min_loss - self.threshold):\n",
    "            self.steps_till_stop += 1\n",
    "            if (self.steps_till_stop == self.patience): return False\n",
    "        return True\n",
    "    \n",
    "    def load_model(self, model):\n",
    "        model.load_state_dict(torch.load(self.path, weights_only=True))\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = (100, 100)\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=SIZE[0], width=SIZE[1]),  \n",
    "    A.RandomGamma(gamma_limit=(80, 120), p=1.0), \n",
    "    A.HorizontalFlip(p = 0.5),\n",
    "    # A.ShiftScaleRotate(p = 0.5),\n",
    "    A.CLAHE(clip_limit=5.0, tile_grid_size=(8, 8), p=1.0), \n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8), \n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "test_transformations = A.Compose([\n",
    "    A.Resize(height=SIZE[0], width=SIZE[1]),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "def compute_train_transformations(image):\n",
    "    image = train_transform(image=image)[\"image\"]\n",
    "    return np.array(image.transpose((2, 0, 1)), dtype=np.float32)\n",
    "    \n",
    "def compute_test_transformations(image):\n",
    "    image = test_transformations(image=image)[\"image\"]\n",
    "    return np.array(image.transpose((2, 0, 1)), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, criterion, optimizer, dataloader : DataLoader):\n",
    "   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   current_training_loss = 0\n",
    "   all_train_labels, all_train_preds = [], []\n",
    "   model.train()\n",
    "   for idx, (images, labels) in enumerate(dataloader):\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      output = model(images)\n",
    "      output = output.to(device)\n",
    "      loss = criterion(output, labels)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      current_training_loss += loss.item()\n",
    "      all_train_preds.extend(output.argmax(dim=1).cpu().numpy())\n",
    "      all_train_labels.extend(labels.cpu().numpy())\n",
    "   loss = current_training_loss / len(dataloader)\n",
    "   acc = round(accuracy_score(all_train_preds, all_train_labels), 3)\n",
    "   return loss, acc\n",
    "\n",
    "def validation_loop(model, criterion, dataloader : DataLoader):\n",
    "   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   all_val_labels, all_val_preds = [], [],\n",
    "   current_validation_loss = 0\n",
    "   model.eval()\n",
    "   with torch.no_grad():\n",
    "      for idx, (images, labels) in enumerate(dataloader):\n",
    "         images, labels = images.to(device), labels.to(device)\n",
    "         output = model(images)\n",
    "         output = output.to(device)\n",
    "         loss = criterion(output, labels)\n",
    "         current_validation_loss += loss.item()\n",
    "         all_val_labels.extend(labels.cpu().numpy())\n",
    "         all_val_preds.extend(output.argmax(dim=1).cpu().numpy())\n",
    "   \n",
    "   loss = current_validation_loss / len(dataloader)\n",
    "   acc =  round(accuracy_score(all_val_labels, all_val_preds), 3)\n",
    "   precision = precision_score(all_val_preds, all_val_labels, average='macro')\n",
    "   recall = recall_score(all_val_preds, all_val_labels, average='macro')\n",
    "   f1 = f1_score(all_val_preds, all_val_labels, average='macro')\n",
    "   return loss, acc, precision, recall, f1  \n",
    "\n",
    "def train_model(parameters, model, data):\n",
    "  earlyStopping : EarlyStopping = parameters[\"early_stopping\"]\n",
    "  current_acc = 0.0\n",
    "  for i in range(parameters[\"epochs\"]):\n",
    "      tloss, tacc = training_loop(model, parameters[\"criterion\"], parameters[\"optimizer\"], data[\"train\"])\n",
    "      mlflow.log_metrics(\n",
    "      metrics={\n",
    "            \"train_loss\": tloss,\n",
    "            \"train_accuracy\": tacc\n",
    "         }\n",
    "      )\n",
    "      vloss, vacc, vprecision, vrecall, vf1 = validation_loop(model, parameters[\"criterion\"], data[\"validation\"])\n",
    "      mlflow.log_metrics(\n",
    "      metrics={\n",
    "          \"validation_loss\": vloss,\n",
    "          \"validation_accuracy\": vacc,\n",
    "          \"validation_precision\": vprecision,\n",
    "          \"validation_recall\": vrecall,\n",
    "          \"validation_f1\": vf1,\n",
    "          }\n",
    "      )\n",
    "      if (earlyStopping != None and not earlyStopping.continue_training(model, vloss)):\n",
    "         print(\"Acuratetea nu a crescut de ceva vreme, a intervenit early stopping\")\n",
    "         break\n",
    "      if (vacc > current_acc): \n",
    "         current_acc = vacc\n",
    "         print(f\"Acuratetea maxima {vacc}, la epoca {i}, loss de {vloss}\")\n",
    "      else:\n",
    "         print(f\"acuratete {vacc}, epoca {i}, loss {vloss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,  width : int, expansion : int):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.width1 = width\n",
    "        self.conv1 = nn.Conv2d(3, self.width1, kernel_size=(3, 3))\n",
    "        self.bn1 = nn.BatchNorm2d(self.width1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "        self.width2 = self.width1 * expansion\n",
    "        self.conv2 = nn.Conv2d(self.width1, self.width2, kernel_size=(3, 3))\n",
    "        self.bn2 = nn.BatchNorm2d(self.width2)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "        self.width3 = self.width2 * expansion\n",
    "        self.conv3 = nn.Conv2d(self.width2, self.width3, kernel_size=(5, 5))\n",
    "        self.bn3 = nn.BatchNorm2d(self.width3)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "        self.width4 = self.width3 * expansion\n",
    "        self.conv4 = nn.Conv2d(self.width3, self.width4, kernel_size=(3, 3))\n",
    "        self.bn4 = nn.BatchNorm2d(self.width4)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "        self.width5 = self.width4 * expansion\n",
    "        self.conv5 = nn.Conv2d(self.width4, self.width5, kernel_size=(3, 3))\n",
    "        self.bn5 = nn.BatchNorm2d(self.width5)\n",
    "\n",
    "        # (1024, 12) -> (1024, 1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classification_layer = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(self.width5, self.width4),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.width4, self.width3),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.width3, 4),\n",
    "            # nn.Softmax(dim=1)\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.maxpool1(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.maxpool2(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.maxpool3(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.maxpool4(self.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.avgpool(self.relu(self.bn5(self.conv5(x))))\n",
    "        return self.classification_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_validation_data(parameters, data_chunks, test_data, current : int):\n",
    "    training_data = []\n",
    "    for idx, chunk in enumerate(data_chunks):\n",
    "        if (idx == current):\n",
    "            continue\n",
    "        training_data += chunk\n",
    "    return {\n",
    "        \"train\": DataLoader(ImagesMRIDataset(training_data, transformations=parameters[\"train_transform\"]), batch_size=parameters[\"batch_size\"], shuffle=True, drop_last=True),\n",
    "        \"validation\": DataLoader(ImagesMRIDataset(data_chunks[current], transformations=parameters[\"test_transform\"]), batch_size=parameters[\"batch_size\"], shuffle=True),\n",
    "        \"test\": DataLoader(ImagesMRIDataset(test_data, transformations=parameters[\"test_transform\"]), batch_size=parameters[\"batch_size\"], shuffle=True)\n",
    "    }\n",
    "\n",
    "def compute_cross_validation(parameters, K, data_chunks, test_info):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    idx = 0\n",
    "    with mlflow.start_run(run_name=f\"fold_{idx}\") as run:\n",
    "        model = Net(width=parameters[\"width\"], expansion=parameters[\"expansion\"])\n",
    "        parameters[\"optimizer\"] = torch.optim.Adam(model.parameters(), lr=parameters[\"lr\"], weight_decay=parameters[\"weight_decay\"])\n",
    "        model = torch.jit.script(model)\n",
    "        model = model.to(device)\n",
    "        mlflow.log_params(parameters)\n",
    "        data = create_cross_validation_data(parameters, data_chunks, test_info, idx)\n",
    "        train_model(parameters, model, data)\n",
    "        if (parameters[\"early_stopping\"]  != None):\n",
    "            best_model = Net(width=parameters[\"width\"], expansion=parameters[\"expansion\"])\n",
    "            parameters[\"early_stopping\"].load_model(best_model)\n",
    "        else:\n",
    "            best_model = model \n",
    "        loss, acc, precision, recall, f1 = validation_loop(best_model, parameters[\"criterion\"], data[\"test\"])\n",
    "        mlflow.log_metrics(\n",
    "            metrics={\n",
    "          \"test_loss\": loss,\n",
    "          \"test_accuracy\": acc,\n",
    "          \"test_precision\": precision,\n",
    "          \"test_recall\": recall,\n",
    "          \"test_f1\": f1,\n",
    "          }\n",
    "        ) \n",
    "        print(f\"pentru test: acuratete {acc}, pierderea este {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea maxima 0.504, la epoca 0, loss de 1.130616762421348\n",
      "Acuratetea maxima 0.685, la epoca 1, loss de 0.7889030738310381\n",
      "Acuratetea maxima 0.743, la epoca 2, loss de 0.6478314860300585\n",
      "Acuratetea maxima 0.821, la epoca 3, loss de 0.5002362050793387\n",
      "acuratete 0.762, epoca 4, loss 0.6370058113878424\n",
      "Acuratetea maxima 0.832, la epoca 5, loss de 0.4426937699317932\n",
      "Acuratetea maxima 0.86, la epoca 6, loss de 0.4337602122263475\n",
      "acuratete 0.845, epoca 7, loss 0.4607644582336599\n",
      "acuratete 0.809, epoca 8, loss 0.5534026514400135\n",
      "acuratete 0.858, epoca 9, loss 0.48577804592522705\n",
      "pentru test valoarea de pierdere 1.6788892894983292 si cea de acuratete 0.706\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "K = 5\n",
    "BALANCED = True\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "training_info, validation_info = split_traing_data(training_info, 0.2)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "\n",
    "training_no_transforms = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "    v2.Resize(SIZE)\n",
    "])\n",
    "\n",
    "# EarlyStopping(path=\"./aici.pth\", patience=5)\n",
    "parameters = {\n",
    "        \"batch_size\": 50,\n",
    "        \"epochs\" : 10,\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"width\": 4,\n",
    "        \"expansion\" : 2,\n",
    "        \"train_transform\": training_no_transforms,\n",
    "        \"test_transform\": training_no_transforms,\n",
    "        \"criterion\": torch.nn.CrossEntropyLoss(),\n",
    "        \"optimizer\": None,\n",
    "        \"scheduler\": None,\n",
    "        \"early_stopping\": None,\n",
    "    }\n",
    "\n",
    "# task 1 - fara augmentari, duplicarea exemplelor din clasele suplimentare\n",
    "mlflow.set_experiment(experiment_name=\"Task1\")\n",
    "compute_cross_validation(parameters, K, data_chunks, test_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alf/anul4/anul4_tot/IS/InvatareSupervizataTema2/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuratetea maxima 0.374, la epoca 0, loss de 1.3199936693364924\n",
      "Acuratetea maxima 0.513, la epoca 1, loss de 1.0807893655516885\n",
      "Acuratetea maxima 0.543, la epoca 2, loss de 1.0228559320623225\n",
      "Acuratetea maxima 0.643, la epoca 3, loss de 0.8228958628394387\n",
      "acuratete 0.606, epoca 4, loss 0.9901242635466836\n",
      "acuratete 0.626, epoca 5, loss 0.8829161308028481\n",
      "Acuratetea maxima 0.657, la epoca 6, loss de 0.7653163888237693\n",
      "Acuratetea maxima 0.74, la epoca 7, loss de 0.6770976077426564\n",
      "acuratete 0.67, epoca 8, loss 0.7841290018775247\n",
      "acuratete 0.691, epoca 9, loss 0.7694819948889993\n",
      "pentru test: acuratete 0.518, pierderea este 1.433277890086174\n"
     ]
    }
   ],
   "source": [
    "K = 5\n",
    "BALANCED = True\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "training_info, validation_info = split_traing_data(training_info, 0.2)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "\n",
    "# task 2 - cu augmentari, cu duplicarea exemplelor din clasele suplimentare\n",
    "parameters = {\n",
    "        \"batch_size\": 50,\n",
    "        \"epochs\" : 10,\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"width\": 4,\n",
    "        \"expansion\" : 2,\n",
    "        \"train_transform\": compute_train_transformations,\n",
    "        \"test_transform\": compute_test_transformations,\n",
    "        \"criterion\": torch.nn.CrossEntropyLoss(),\n",
    "        \"optimizer\": None,\n",
    "        \"scheduler\": None,\n",
    "        \"early_stopping\": None,\n",
    "    }\n",
    "\n",
    "mlflow.set_experiment(experiment_name=\"Task2\")\n",
    "compute_cross_validation(parameters, K, data_chunks, test_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "BALANCED = False\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "training_info, validation_info = split_traing_data(training_info, 0.2)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "\n",
    "def compute_class_weights(data_stream):\n",
    "    labels = [label for _, label in data_stream]\n",
    "    label_counts = Counter(labels)\n",
    "    total_samples = sum(label_counts.values())\n",
    "    class_weights = {label: total_samples / count for label, count in label_counts.items()}\n",
    "    max_label = max(label_counts.keys())\n",
    "    weights_list = [class_weights.get(i, 0.0) for i in range(max_label + 1)]\n",
    "    weights = torch.tensor(weights_list, dtype=torch.float)\n",
    "    print(weights)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    weights = weights.to(device)\n",
    "    return weights \n",
    "\n",
    "\n",
    "def compute_functions_criterion_weighted(model : Net, parameters):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=parameters[\"lr\"], weight_decay=parameters[\"weight_decay\"])\n",
    "    return torch.nn.CrossEntropyLoss(weight=compute_class_weights(training_info)), optimizer, None, None\n",
    "\n",
    "# Task 2 cu augmentari - cu functia de loss avand weights\n",
    "task2_augmentation_test_weighted, task2_augmentation_validation_weighted = compute_cross_validation(current_parameters, K, data_chunks, test_info, compute_train_transformations, compute_test_transformations, compute_functions_criterion_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import Resize, Rand2DElastic, RandAdjustContrast, RandRotate, ScaleIntensity, Compose\n",
    "from monai.transforms import RandGaussianNoise, RandAffine, RandStdShiftIntensity\n",
    "from monai.transforms import RandGaussianSmooth, ThresholdIntensity, RandAxisFlip\n",
    "from monai.transforms import RandSpatialCrop, RandGaussianSharpen, RandHistogramShift, RandShiftIntensity\n",
    "import cv2\n",
    "\n",
    "first_augmentation_pipeline = Compose([\n",
    "    Resize(spatial_size= SIZE),\n",
    "    RandGaussianSharpen(),\n",
    "    RandAxisFlip(prob=0.3),\n",
    "    RandSpatialCrop(roi_size=SIZE, random_center=True, random_size=False),\n",
    "    RandRotate(range_x=(-50, 50),  prob=0.5, keep_size=True),\n",
    "    RandAdjustContrast(prob=0.8),\n",
    "    ScaleIntensity(minv=0.0, maxv=1.0)\n",
    "])\n",
    "\n",
    "second_augmentation_pipeline = Compose([\n",
    "    Resize(spatial_size= SIZE),\n",
    "    Rand2DElastic(spacing=(5, 10), magnitude_range=(1, 2), prob=0.5, rotate_range=(0, 0), shear_range=(0.02, 0.02),\n",
    "                    translate_range=(10, 10), scale_range=(0.1, 0.1), spatial_size=SIZE, mode=\"bilinear\", padding_mode=\"reflection\"),\n",
    "    RandHistogramShift(prob=0.8),\n",
    "    ThresholdIntensity(threshold=20),\n",
    "    RandGaussianSmooth(sigma_x=(0.5, 1.5), sigma_y=(0.5, 1.5), prob=0.8),\n",
    "    RandRotate(range_x=(-50, 50),  prob=0.5, keep_size=True),\n",
    "    ScaleIntensity(minv=0.0, maxv=1.0)\n",
    "])\n",
    "\n",
    "\n",
    "third_augmentation_pipeline = Compose([\n",
    "    Resize(spatial_size= SIZE),\n",
    "    RandGaussianNoise(mean=0.0, std=0.1, prob=0.8),\n",
    "    RandShiftIntensity(offsets=(-20, 20), prob=0.3, safe=True),\n",
    "    RandAxisFlip(prob=0.3),\n",
    "    RandStdShiftIntensity(factors=0.8),\n",
    "    RandAffine(prob=0.2),\n",
    "    ScaleIntensity(minv=0.0, maxv=1.0)\n",
    "])\n",
    "\n",
    "test_augmentation_pipeline = Compose([\n",
    "    Resize(spatial_size= SIZE),\n",
    "    ScaleIntensity(minv=0.0, maxv=1.0)\n",
    "])\n",
    "\n",
    "def create_test_monai(img):\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return test_augmentation_pipeline(img)\n",
    "\n",
    "def create_first_monai_transform(img):\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return first_augmentation_pipeline(img)\n",
    "\n",
    "\n",
    "def create_second_monai_transform(img):\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return second_augmentation_pipeline(img)\n",
    "\n",
    "\n",
    "def create_third_monai_transform(img):\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return third_augmentation_pipeline(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "BALANCED = True\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "training_info, validation_info = split_traing_data(training_info, 0.2)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "task3_first_monai_test, task3_first_monai_validation = compute_cross_validation(current_parameters, K, data_chunks, test_info, create_first_monai_transform, create_test_monai, compute_functions_balanced_no_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task3_second_monai_test, task3_second_monai_validation = compute_cross_validation(current_parameters, K, data_chunks, test_info, create_second_monai_transform, create_test_monai, compute_functions_balanced_no_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task3_third_monai_test, task3_third_monai_validation = compute_cross_validation(current_parameters, K, data_chunks, test_info, create_third_monai_transform, create_test_monai, compute_functions_balanced_no_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_functions_with_early_stopper(model : Net, parameters):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=parameters[\"lr\"], weight_decay=parameters[\"weight_decay\"])\n",
    "    return torch.nn.CrossEntropyLoss(), optimizer, None, EarlyStopping(\"./aici.pth\", parameters['early_stopping_patience'])\n",
    "\n",
    "def compute_functions_simple(model : Net, parameters):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=parameters[\"lr\"], weight_decay=parameters[\"weight_decay\"])\n",
    "    return torch.nn.CrossEntropyLoss(), optimizer, torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', \n",
    "                patience=parameters[\"optimizer_patience\"], factor=parameters[\"lr_factor\"]), EarlyStopping(\"./aici.pth\", parameters['early_stopping_patience'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "BALANCED = True\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "training_info, validation_info = split_traing_data(training_info, 0.2)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "current_parameters = {\n",
    "    \"batch_size\": 50,\n",
    "    \"epochs\" : 50,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"optimizer_patience\" : 3,\n",
    "    \"lr_factor\": 0.6,\n",
    "    \"width\": 16,\n",
    "    \"expansion\" : 4,\n",
    "    \"early_stopping_patience\" : 7,\n",
    "}\n",
    "\n",
    "task4_ealry_stopping_test, task4_ealry_stopping_validation = compute_cross_validation(current_parameters, K, data_chunks, test_info, create_third_monai_transform, create_test_monai, compute_functions_with_early_stopper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "BALANCED = True\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "training_info, validation_info = split_traing_data(training_info, 0.2)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "current_parameters = {\n",
    "    \"batch_size\": 50,\n",
    "    \"epochs\" : 25,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"optimizer_patience\" : 3,\n",
    "    \"lr_factor\": 0.6,\n",
    "    \"width\": 16,\n",
    "    \"expansion\" : 4,\n",
    "    \"early_stopping_patience\" : 7,\n",
    "}\n",
    "\n",
    "def compute_functions_with_lr_scheduler(model : Net, parameters):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=parameters[\"lr\"], weight_decay=parameters[\"weight_decay\"])\n",
    "    return torch.nn.CrossEntropyLoss(), optimizer, torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', \n",
    "                patience=parameters[\"optimizer_patience\"], factor=parameters[\"lr_factor\"]), None\n",
    "\n",
    "\n",
    "task4_lr_scheduler_test, task4_lr_scheduler_validation = compute_cross_validation(current_parameters, K, data_chunks, test_info, create_first_monai_transform, create_test_monai, compute_functions_with_lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5\n",
    "BALANCED = True\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "training_info, validation_info = split_traing_data(training_info, 0.2)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "parameters = {\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\" : 50,\n",
    "    \"lr\": 1e-2,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"optimizer_patience\" : 3,\n",
    "    \"lr_factor\": 0.6,\n",
    "    \"width\": 16,\n",
    "    \"expansion\" : 4,\n",
    "    \"early_stopping_patience\" : 7,\n",
    "}\n",
    "\n",
    "# 72 maxim\n",
    "def compute_ablatiations(model : Net, parameters):\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=parameters[\"lr\"], weight_decay=parameters[\"weight_decay\"])\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=parameters[\"lr\"], weight_decay=parameters[\"weight_decay\"], momentum=0.7)\n",
    "    return torch.nn.BCEWithLogitsLoss(), optimizer, torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', \n",
    "                patience=parameters[\"optimizer_patience\"], factor=parameters[\"lr_factor\"]), EarlyStopping(\"./aici.pth\", patience=parameters[\"early_stopping_patience\"])\n",
    "\n",
    "SIZE=(100, 100)\n",
    "ablatiation_augmentation = Compose([\n",
    "    Resize(spatial_size= SIZE),\n",
    "    RandGaussianSharpen(),\n",
    "    RandAxisFlip(prob=0.3),\n",
    "    RandSpatialCrop(roi_size=SIZE, random_center=True, random_size=False),\n",
    "    RandRotate(range_x=(-50, 50),  prob=0.5, keep_size=True),\n",
    "    RandAdjustContrast(prob=0.8),\n",
    "    ScaleIntensity(minv=0.0, maxv=1.0)\n",
    "])\n",
    "\n",
    "\n",
    "def create_ablatiation_transform(img):\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return ablatiation_augmentation(img)\n",
    "\n",
    "\n",
    "test_augmentation_ablatiation = Compose([\n",
    "    Resize(spatial_size= SIZE),\n",
    "    ScaleIntensity(minv=0.0, maxv=1.0)\n",
    "])\n",
    "\n",
    "def create_ablatiation_test_monai(img):\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    return test_augmentation_ablatiation(img)\n",
    "\n",
    "\n",
    "validation_info = {\"precision\": [], \"recall\": [], \"f1Score\" : [], \"accuracy\" : []} \n",
    "info_test = {\"precision\": [], \"recall\": [], \"f1Score\" : [], \"accuracy\" : []}\n",
    "for i in range(K):\n",
    "    if (i != 1):\n",
    "        continue\n",
    "    model = Net(width=parameters[\"width\"], expansion=parameters[\"expansion\"])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion, optimizer, lr_scheduler, early_stopping = compute_ablatiations(model, parameters)\n",
    "    data = create_cross_validation_data(data_chunks, test_info, i, create_ablatiation_transform, create_ablatiation_test_monai, parameters[\"epochs\"])\n",
    "    training_loss, validation_loss, training_accuracy, validation_accuracy = train_model(model, parameters[\"epochs\"],\n",
    "            data, criterion, optimizer, lr_scheduler, early_stopping)\n",
    "    if (early_stopping != None):\n",
    "        best_model = Net(width=parameters[\"width\"], expansion=parameters[\"expansion\"])\n",
    "        early_stopping.load_model(best_model)\n",
    "    else:\n",
    "        best_model = model\n",
    "    precision, recall, f1, acc, mat = test_model(best_model, data[\"validation\"], criterion)\n",
    "    precision, recall, f1, acc, mat = test_model(best_model, data[\"test\"], criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
