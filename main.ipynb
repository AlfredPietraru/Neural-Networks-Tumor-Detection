{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri\n",
      "License(s): MIT\n",
      "Downloading brain-tumor-classification-mri.zip to /home/alf/anul4/InvatareSupervizataTema2\n",
      "100%|██████████████████████████████████████| 86.8M/86.8M [00:11<00:00, 8.58MB/s]\n",
      "100%|██████████████████████████████████████| 86.8M/86.8M [00:11<00:00, 8.11MB/s]\n"
     ]
    }
   ],
   "source": [
    "#!/bin/bash\n",
    "# https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri/data\n",
    "!kaggle datasets download sartajbhuvaji/brain-tumor-classification-mri\n",
    "!unzip \"./brain-tumor-classification-mri.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, precision_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from aux import ImagesMRIDataset, split_for_cross_validation, get_training_testing_data, plot_data, get_data_distribution, split_traing_data\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, path : str, patience=5, threshold=1e-4):\n",
    "        self.patience = patience\n",
    "        self.threshold = threshold\n",
    "        self.min_loss = 10000\n",
    "        self.steps_till_stop = 0\n",
    "        self.path = path\n",
    "\n",
    "    def continue_training(self, model, loss):\n",
    "        if(loss < self.min_loss - self.threshold):\n",
    "            self.min_loss = loss\n",
    "            self.steps_till_stop = 0\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            return True\n",
    "        if (loss >= self.min_loss - self.threshold):\n",
    "            self.steps_till_stop += 1\n",
    "            if (self.steps_till_stop == self.patience): return False\n",
    "        return True\n",
    "    \n",
    "    def load_model(self, model):\n",
    "        model.load_state_dict(torch.load(self.path, weights_only=True))\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = (100, 100)\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=SIZE[0], width=SIZE[1]),  \n",
    "    A.RandomGamma(gamma_limit=(80, 120), p=1.0),\n",
    "    A.AdvancedBlur(blur_limit=(3, 5), p=0.5),  \n",
    "    A.HorizontalFlip(p = 0.5),\n",
    "    A.CLAHE(clip_limit=5.0, tile_grid_size=(8, 8), p=1.0), \n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8), \n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "test_transformations = A.Compose([\n",
    "    A.Resize(height=SIZE[0], width=SIZE[1]),\n",
    "    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "def compute_train_transformations(image):\n",
    "    image = train_transform(image=image)[\"image\"]\n",
    "    return np.array(image.transpose((2, 0, 1)), dtype=np.float32)\n",
    "    \n",
    "def compute_test_transformation(image):\n",
    "    image = test_transformations(image=image)[\"image\"]\n",
    "    return np.array(image.transpose((2, 0, 1)), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, criterion, optimizer, dataloader : DataLoader):\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  current_training_loss = 0\n",
    "  all_train_labels, all_train_preds = [], []\n",
    "  model.train()\n",
    "  for idx, (images, labels) in enumerate(dataloader):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    output = model(images)\n",
    "    output = output.to(device)\n",
    "    loss = criterion(output, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    current_training_loss += loss.item()\n",
    "    all_train_preds.extend(output.argmax(dim=1).cpu().numpy())\n",
    "    all_train_labels.extend(labels.cpu().numpy())\n",
    "  return current_training_loss / dataloader.dataset.__len__(),  all_train_preds, all_train_labels\n",
    "\n",
    "\n",
    "def validation_loop(model, criterion, dataloader : DataLoader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    all_val_labels, all_val_preds = [], [],\n",
    "    current_validation_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for idx, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        output = model(images)\n",
    "        output = output.to(device)\n",
    "        loss = criterion(output, labels)\n",
    "        current_validation_loss += loss.item()\n",
    "        all_val_labels.extend(labels.cpu().numpy())\n",
    "        all_val_preds.extend(output.argmax(dim=1).cpu().numpy())\n",
    "    return current_validation_loss / dataloader.dataset.__len__(),  all_val_preds, all_val_labels\n",
    "\n",
    "def train_model(model, epochs, data, criterion, optimizer, lr_scheduler, early_stopping : EarlyStopping):\n",
    "  training_loss = [] \n",
    "  validation_loss = []\n",
    "  training_accuracy = []\n",
    "  validation_accuracy = []\n",
    "  maxim_accuracy = -1\n",
    "  for i in range(epochs):\n",
    "      Tloss, train_pred, train_labels = training_loop(model, criterion, optimizer, data[\"train\"])\n",
    "      Vloss, all_val_preds, all_val_labels = validation_loop(model, criterion, data[\"validation\"])\n",
    "      if (not early_stopping.continue_training(model, Vloss)):\n",
    "         print(\"Loss-ul nu a scazut de ceva vreme, a intervenit early stopping\")\n",
    "         break\n",
    "      lr_scheduler.step(Vloss)\n",
    "      time.sleep(2)\n",
    "      training_loss.append(round(Tloss, 3))\n",
    "      validation_loss.append(round(Vloss, 3))\n",
    "      training_accuracy.append(round(accuracy_score(train_pred, train_labels), 3))\n",
    "      validation_accuracy.append(round(accuracy_score(all_val_labels, all_val_preds), 3))\n",
    "      if (maxim_accuracy < validation_accuracy[-1]):\n",
    "        print(f\"best model found at {i}\", f\"loss is {validation_loss[-1]}\", f\"accuracy is {validation_accuracy[-1]}\")\n",
    "        maxim_accuracy = validation_accuracy[-1]\n",
    "      else:\n",
    "         print(f\"epoca {i}\", f\"loss is {validation_loss[-1]}\", f\"accuracy is {validation_accuracy[-1]}\")\n",
    "  return training_loss, validation_loss, training_accuracy, validation_accuracy\n",
    "\n",
    "def test_model(model, dataloader, criterion):\n",
    "    _, preds, labels = validation_loop(model, criterion, dataloader)\n",
    "    precision = precision_score(preds, labels, average='macro') \n",
    "    recall = recall_score(preds, labels, average='macro')\n",
    "    f1 = f1_score(preds, labels, average='macro')\n",
    "    acc = accuracy_score(preds, labels)\n",
    "    return precision, recall, f1, acc, confusion_matrix(preds, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,  width : int, expansion : int):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.width1 = width\n",
    "        self.conv1 = nn.Conv2d(3, self.width1, kernel_size=(3, 3))\n",
    "        self.bn1 = nn.BatchNorm2d(self.width1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "        self.width2 = self.width1 * expansion\n",
    "        self.conv2 = nn.Conv2d(self.width1, self.width2, kernel_size=(3, 3))\n",
    "        self.bn2 = nn.BatchNorm2d(self.width2)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "        self.width3 = self.width2 * expansion\n",
    "        self.conv3 = nn.Conv2d(self.width2, self.width3, kernel_size=(5, 5))\n",
    "        self.bn3 = nn.BatchNorm2d(self.width3)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "\n",
    "        self.width4 = self.width3 * expansion\n",
    "        self.conv4 = nn.Conv2d(self.width3, self.width4, kernel_size=(3, 3))\n",
    "        self.bn4 = nn.BatchNorm2d(self.width4)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classification_layer = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(self.width4, self.width3),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.width3, self.width2),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.width2, 4),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x : torch.tensor):\n",
    "        x = self.maxpool1(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.maxpool2(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.maxpool3(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.avgpool(x)\n",
    "        return self.classification_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(list1, list2, label1, label2, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(x=range(1, len(list1) + 1), y=list1, label=label1, marker='o')\n",
    "    sns.lineplot(x=range(1, len(list2) + 1), y=list2, label=label2, marker='o')\n",
    "    plt.xlabel(label1)   \n",
    "    plt.ylabel(label2)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_matrix(matrix):\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix = matrix, display_labels = [0, 1, 2, 3])\n",
    "    cm_display.plot()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def add_info(info_data, precision, acc, recall, f1):\n",
    "    info_data[\"precision\"].append(precision)\n",
    "    info_data[\"accuracy\"].append(acc)\n",
    "    info_data[\"recall\"].append(recall)\n",
    "    info_data[\"f1Score\"].append(f1)\n",
    "    return info_data\n",
    "\n",
    "\n",
    "def create_cross_validation_data(data_chunks, test_data, current : int, train_transform, test_transform, batch_size):\n",
    "    training_data = []\n",
    "    for idx, chunk in enumerate(data_chunks):\n",
    "        if (idx == current):\n",
    "            continue\n",
    "        training_data += chunk\n",
    "    return {\n",
    "        \"train\": DataLoader(ImagesMRIDataset(training_data, transformations=train_transform), batch_size=batch_size, shuffle=True, drop_last=True),\n",
    "        \"validation\": DataLoader(ImagesMRIDataset(data_chunks[current], transformations=test_transform), batch_size=batch_size, shuffle=True),\n",
    "        \"test\": DataLoader(ImagesMRIDataset(test_data, transformations=test_transform), batch_size=batch_size, shuffle=True)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2\n",
    "K = 5\n",
    "BALANCED = True\n",
    "training_info, test_info = get_training_testing_data(BALANCED)\n",
    "training_info, validation_info = split_traing_data(training_info, 0.2)\n",
    "data_chunks = split_for_cross_validation(training_info, K)\n",
    "\n",
    "parameters = {\n",
    "    \"batch_size\" : 5,\n",
    "    \"epochs\" : 50,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"optimizer_patience\" : 3,\n",
    "    \"lr_factor\": 0.1,\n",
    "    \"width\": 16,\n",
    "    \"expansion\" : 4,\n",
    "    \"early_stopping_patience\" : 7,\n",
    "}\n",
    "\n",
    "def compute_functions(model : Net, parameters):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=parameters[\"lr\"], weight_decay=parameters[\"weight_decay\"])\n",
    "    return torch.nn.NLLLoss(), optimizer, torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', \n",
    "                patience=parameters[\"optimizer_patience\"], factor=parameters[\"lr_factor\"])\n",
    "\n",
    "info_test = {\"precision\": [], \"recall\": [], \"f1Score\" : [], \"accuracy\" : []}\n",
    "for i in range(K):\n",
    "    model = Net(width=parameters[\"width\"], expansion=parameters[\"expansion\"])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    criterion, optimizer, lr_scheduler = compute_functions(model, parameters)\n",
    "    data = create_cross_validation_data(data_chunks, test_info, i, compute_train_transformations, compute_test_transformation, parameters[\"epochs\"])\n",
    "    early_stopping = EarlyStopping(\"./aici.pth\", parameters['early_stopping_patience'])\n",
    "    training_loss, validation_loss, training_accuracy, validation_accuracy = train_model(model, parameters[\"epochs\"],\n",
    "        data, criterion, optimizer, lr_scheduler, early_stopping)\n",
    "    show_plot(training_loss, validation_loss, \"Train Loss\", \"Validation Loss\", f\"Loss model{i}\")\n",
    "    show_plot(training_accuracy, validation_accuracy, \"Train Accuracy\", \"Validation Accuracy\", f\"Accuracy model{i}\")\n",
    "    best_model = Net(width=parameters[\"width\"], expansion=parameters[\"expansion\"])\n",
    "    early_stopping.load_model(best_model)\n",
    "    precision, recall, f1, acc, mat = test_model(best_model, data[\"test\"], criterion)\n",
    "    info_test = add_info(info_test, precision, acc, recall, f1)\n",
    "    break\n",
    "\n",
    "test_results = pd.DataFrame.from_dict(info_test, \"columns\")\n",
    "print(test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
